---
title: Netflix description analysis
author: Emanuel Michele Soda
date: '2022-08-06'
slug: netflix-description-analysis
categories: []
tags:
  - netflix
  - analysis
editor_options: 
  chunk_output_type: console
---

# Introduction

## NPL a super brief introduction

In this post we are going to have a look to what is called **Natural Language Processing** also known as **NLP**. First of all let's define what is NLP. According to [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing):

> ***Natural language processing** (**NLP**) is a subfield of [linguistics](https://en.wikipedia.org/wiki/Linguistics "Linguistics"), [computer science](https://en.wikipedia.org/wiki/Computer_science "Computer science"), and [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence "Artificial intelligence") concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of [natural language](https://en.wikipedia.org/wiki/Natural_language "Natural language") data. The goal is a computer capable of "understanding" the contents of documents, including the [contextual](https://en.wikipedia.org/wiki/Context_(language_use) "Context (language use)") nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.*

In brief NPL is a set of tools able to extract information from text data.

For this very introductory tutorial we will analyse the description of **Netflix** movies[^1] and trying to predict the film category using the description. In order to perform this task we will use above all those packages:

[^1]: More information can be found here [Netflix Shows](https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-04-20)

-   `tidyverse` for the data manipulation

-   `tidymodels` for the modelling

-   `tidytext` for the analysis of the text data

-   `textrecipes` for additional recipes step for text data

## Importing the pacakges

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(tidytext)
library(textrecipes)
library(hrbrthemes)
theme_set(theme_light(base_size = 20))
```

## Read data

```{r include=FALSE}
path <- "/Users/ieo5571/Documents/blog_data_science_bioinfo/Data/tuesdata.csv"
```

The data is in *.csv* in order to read it we can use the `readr::read_csv()` function and save them in the `netflix_titles` variable.

```{r}
netflix_titles <- readr::read_csv(path)
```

In order to have a look to the data we can use a very handy function from the `dplyr` which is `glimpse()`.

```{r}
dplyr::glimpse(netflix_titles)
```

We can see that we have 7,787 rows and 12 columns. It seams also that the dataset contains also *TV Shows.* We can filter them out and select only the following columns:

-   `title`

-   `description`

-   `listed_in`

Moreover using `tidyr::separate_rows()` we can split the word of the `description` column and create a table with a row for each word.

```{r}
netflix_films <- 
  netflix_titles %>%  
  filter(tolower(type)  == "movie") %>% 
  select(title, description, listed_in) %>% 
  separate_rows(listed_in, sep = ", ")
```

## Exploratory Data Analysis

Let's first of all perform some **Exploratory Data Analysis** (**EDA**). A useful information could be counting the number of films by category. This can be done as follows:

```{r}
netflix_films %>%
  count(listed_in, sort = T) %>%
  mutate(listed_in = fct_reorder(listed_in, n)) %>%
  ggplot(aes(x = n, y = listed_in, fill = n)) +
  geom_segment(aes(xend = 0, yend = listed_in),
    color = "grey50"
  ) +
  geom_point(aes(size = abs(n)),
    shape = 21,
    color = "grey50",
    show.legend = FALSE
  ) +
  scale_fill_viridis_c(option = "C") +
  scale_size_continuous(range = c(4, 7)) +
  labs(x = "Number of films", y = NULL)
```

We have lots of categories!!!

Some of the categories have lots of observations, while other very few. This is not the optimal condition for a *machine learning model*.

We can follow different path here:

-   We can select some categories and collapse all the other into a macro category

-   We can select just few of them and drop all the others.

For the sake of this simple tutorial we will select just some categories which are:

-   Horror Movies

-   Children & Family Movies

-   Documentaries

In doing so we are left with 1,630.

```{r}
category_to_select <- 
    c("Horror Movies", "Children & Family Movies",
      "Documentaries")

netflix_films <- netflix_films %>%  
  filter(listed_in %in% category_to_select)

netflix_films %>%  
    count(listed_in, sort = TRUE) %>% 
    knitr::kable(align = "lccrr",
                 caption = "Number of film by category")
    
```

As can be seen the number of observations for each category is a bit different. We can ask ourself if the dataset is balanced or not. In order to answer this question we can use the **Shannon Equitability Index which is a popular index to assess diversity. It is computed as follows:**

$$
E_h=- \frac{\sum _{i=1}^{R}p_{i}\ln p_{i}}{\ln S}
$$

where:

-   *p~i~* is the proportion of the samples belonging to the *i*th group

-   *S* is the total number of groups

```{r}
shannon_index <- function(data_frame, class){
  result <- data_frame %>% 
    count({{ class }}) %>% 
    mutate(check =  - (n / sum(n) * log(n / sum(n))) / 
             log(length({{ class }}))) %>% 
    summarise(balance = sum(check))  %>% 
    pull(balance)
    return(result)
}

netflix_films %>%  
    count(listed_in) %>% 
    mutate(sum_n = sum(n)) %>% 
    mutate(check =  - (n / sum(n) * log(n / sum(n) ))) %>% 
    summarise(balance = sum(check)) 

shannon_index_netflix <- netflix_films %>% 
  shannon_index(class = listed_in) %>%  
    round(digits = 2)

netflix_films  %>% 
  mutate(film = "film") %>% 
  group_by(listed_in) %>% 
  add_count() %>% 
  ggplot(aes(film, fill = listed_in, label = n)) +
  geom_bar(position = position_fill(), color = "black") +
  scale_fill_ipsum() +
  scale_y_percent(n.breaks = 8) +
  labs(fill = NULL, x = NULL, y = "Percentage of film",
       title = "Number of film per category",
       subtitle  = paste0(" Shannon Equitability Index: ",
                          shannon_index_netflix)) + 
  theme(legend.position = "bottom",
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        text = element_text(size = 18)) +
  coord_flip()  
```

According to the **Shannon Equitability Index** the dataset is balanced. For this reason we can go on with our analysis.

We are now going to count the number of words by category. Ideally words that are recurrent in a category will be very useful in the modelling part.

As can be seen from the plot the most recurrent word in a category is *documentary . As expected* it is present a lot in documentary.

We can also notice that we have a problem with words like **find** and **finds**. This is a **stemming problem** but for the sake of this tutorial we will ignore this problem. If you are interested in what is stemming here the link to [the Wikipedia page](https://en.wikipedia.org/wiki/Stemming).

```{r fig.height=10}
word_by_category <- 
  netflix_films %>% 
  unnest_tokens(word, description)  %>%
  anti_join(get_stopwords(), by = "word")  %>% 
  count(word, listed_in,  sort = TRUE)   %>% 
  group_by(listed_in) %>% 
  slice_max(n = 15, order_by = n) %>%  
  ungroup() %>% 
  add_count(listed_in, name = "n_listed_in") %>%  
  mutate(word = reorder_within(x = word, by = n, 
                               within = listed_in)) 
  

ggplot(word_by_category, aes(n, word, fill = listed_in)) +
  geom_segment(aes(xend = 0, yend = word), color = "grey50") +
  geom_point(aes(size = abs(n)), shape = 21, color = "grey50", 
             show.legend = FALSE) + 
  facet_wrap(vars(listed_in), scales = "free_y", ncol = 1) +
  scale_y_reordered() + 
  scale_fill_ipsum() +
  labs(y = NULL, x = "Number of word") +
  theme(text = element_text(size = 18)) +
  scale_size_continuous(range = c(3, 10))

```

## Unsupervised Analysis

Finally we can start to model our data. The first step is to split our data in training and testing set, in addition in we will create some resampling folds to perform the cross validation.

```{r}
set.seed(123)
films_split <- initial_split(netflix_films, strata =  listed_in)

films_train <- training(films_split)
films_test <- testing(films_split)

set.seed(42)
films_folds <- vfold_cv(films_train, strata = listed_in)
```

From now on we will only work on the training `films_train` and `films_test`will be used just to assess the quality of our model on new data at the end on the analysis.

A first type of analysis that we can make is performing a Principal Component Analysis. PCA is an unsupervised modelling analysis in which the original space generated by the predictors is transformed into another space in which the variance is maximized along the axis. On top of this PCA has another very interesting properties, the first PC is the one with the highest explained variance, the second PC is the second with the highest explained variance and so on.

But, in order to use PCA we have to transform the word in a numerical matrix. A very effective way to do that is using **tf-idf**.

In a very simple way **tf** e **idf.** The $tf$ part is defined as follow:

$$
{tf_{i,j}} ={\frac {n_{i,j}}{|d_{j}|}}
$$

-   $n_{i,j}$ is the number of occurrence of the term $i$ in the document $j$

-   $|d_{j}|$ is the dimension of the document expressed in number of terms

While the $idf$ parte is comuted as follow:

$${idf_{i}} =\log _{10}{\frac {|D|}{|\{d:i\in d\}|}}$$

-   $|D|$ is the number of document in the collection

-   $|d_{j}|$ is the number of the document that contains the $i$ document

In the end $tf-idf$ is computed as:

$$
\mathrm {(tf{\mbox{-}}idf)_{i,j}} =\mathrm {tf_{i,j}} \times \mathrm {idf_{i}} .
$$

As can be seen this matrix is very sparse and this is typical of text data.

```{r include=FALSE}
data <- recipe(listed_in ~ description, films_train) %>% 
    step_tokenize(description, 
                  options = list(lowercase = TRUE)) %>% 
    step_stopwords(all_predictors()) %>% 
    step_tokenfilter(all_predictors()) %>% 
    step_tfidf(all_predictors())  %>%  
    prep() %>%  
    juice()

data[100:105, 1:5]  %>% 
    knitr::kable(caption = "Word count matrix")
    
```

```{r}
films_rep_pca <- 
  recipe(listed_in ~ description, films_train) %>% 
  step_tokenize(description, options = list(lowercase = TRUE)) %>%
  step_stopwords(all_predictors()) %>% 
  step_tokenfilter(all_predictors()) %>% 
  step_tfidf(all_predictors())  %>% 
  step_normalize(all_numeric_predictors())  %>% 
  step_pca(all_numeric_predictors(), num_comp = 10) 
  

films_juice_pca <- 
  films_rep_pca %>% 
  prep() %>% 
  juice()
```

Looking at the **PCA** we can see that the first component divides quite well the Documentary and the Children & Family Movies while the second component divides Horror movies from the other two category.

```{r}
films_juice_pca %>% 
  ggplot(aes(PC01, PC02, fill = listed_in)) +
  geom_point(shape = 21, size = 8) +
  scale_fill_ipsum(guide = 
                       guide_legend(title = NULL, 
                                    override.aes =
                                        list(size = 10))) +
  labs(x = "Principal component 1 (PC01)", 
       y = "Principal component 2 (PC02)") +
  theme(legend.position = "top",
        text = element_text(size = 15)) 
```

```{r}
films_juice_pca %>% 
  pivot_longer(PC01:PC02) %>% 
  mutate(name = 
           if_else(name == "PC01", 
                   "Principal component 1\n(PC01)",
                   "Principal component 2\n(PC02)")) %>% 
  ggplot(aes(name, value)) +
  geom_violin(aes(fill = listed_in), 
              position = position_dodge(0.8), alpha = 0.8) +
  geom_boxplot(aes(color = listed_in),  show.legend = FALSE,
               position = position_dodge(0.8), width = 0.1) +
  scale_fill_ipsum() +
  labs(fill = NULL, x = NULL, y = NULL) +
  theme(legend.position = "top")  +
  scale_color_manual(values = rep("black", 3))
```

From the PCA we can obtain other information. Each principal component is a linear combination of the original columns. We can extract for each PC the word by which it is created by looking at the loading.

```{r fig.height=14}
pca_loading <- 
  tidy(films_rep_pca %>% prep(), number = 6, type = "coef") %>% 
  filter(component %in% paste0("PC", 1:2)) %>% 
  mutate(terms = str_remove(terms, "tfidf_description_"))  %>% 
  mutate(sign = if_else(value > 0, "Positive", "Negative")) 

pca_loading %>%  
  mutate(label = if_else(abs(value) > 0.25, 
                         round(value, 1), NULL)) %>% 
  group_by(component) %>% 
  slice_max(n = 25, order_by = abs(value)) %>% 
  mutate(terms = reorder_within(terms, value, component)) %>% 
  ggplot(aes(x = value, y = terms, fill = sign)) +
  geom_segment(aes(xend = 0, yend = terms), color = "grey50") +
  geom_point(aes(size = abs(value)), shape = 21, 
             color = "grey50", show.legend = FALSE) +
  geom_text(aes(label = label), size = 2.5) +
  facet_wrap(vars(component), scales = "free_y", ncol = 1) + 
  scale_fill_manual(values = c("#de2d26", "#31a354")) +
  labs(x = "Loading", y = NULL, 
       title = "Top 25 word by component") +
  scale_size_continuous(range = c(2, 10)) +
  scale_y_reordered()  + 
  theme(text = element_text(size = 15))
```

## Modelling

Using the `recipes` a simple preprocessing recipe can be created. Then a model in this case a we are using linear support vector machine which works very well with text is created.

```{r}
films_rep <- 
  recipe(listed_in ~ ., films_train) %>% 
  update_role(title, new_role = "ID") %>%
  step_tokenize(description, options = list(lowercase = TRUE)) %>%
  step_stopwords(all_predictors()) %>% 
  step_tokenfilter(all_predictors()) %>% 
  step_tfidf(all_predictors()) %>% 
  step_normalize(all_predictors())
  
films_rep
```

```{r}
svm_spec <- svm_linear(cost = 0.5) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_spec
```

In order to train the model on our dataset we can take advantage of the `tidymodel` ecosystem and create a `workflow object` which can be seen as a place to store both preprocessing step and a model.

```{r echo=TRUE}
films_wf <- workflow() %>%
  add_recipe(films_rep) 
films_wf

doParallel::registerDoParallel(cores = 8)

set.seed(1234)
sv_film_res <- films_wf %>% 
  add_model(svm_spec) %>%
  fit_resamples(
    resamples = films_folds,
    metrics = metric_set(roc_auc, accuracy, sens, spec),
    control = control_grid(save_pred = TRUE, verbose = TRUE)
  )
```

```{r include=FALSE}
saveRDS(sv_film_res, "/Users/ieo5571/Documents/blog_data_science_bioinfo/Data/sv_film_res.RData")
```

The accuracy achieved by this simple model is $68\%$ which is not extraordinary but is anyway good enough considering the little effort we have put. Moreover, if we create an empty model we obtain only an accuracy of $48\%$ .

What is very good is that this model achieves a very high *specificity* which means that we have very low *false positive.* Which means that most of the time if for example it says "Documentary" it is actually a Documentary.

```{r}
# NUll model

model_null <- null_model() %>% 
  set_engine("parsnip") %>% 
  set_mode("classification")

set.seed(1234)
workflow() %>% 
    add_recipe(films_rep) %>%
    add_model(model_null) %>%  
    fit_resamples(
        resamples = films_folds,
        metrics = metric_set(roc_auc, accuracy, sens, spec),
        control = control_grid(save_pred = TRUE, verbose = TRUE)
    )  %>%  
    collect_metrics()
```

```{r}
sv_film_res_metrics <-
    collect_metrics(sv_film_res) %>% 
  select(.metric, mean, std_err)  %>% 
  mutate(.metric = case_when(
    .metric == "roc_auc" ~ "roc auc",
    .metric == "sens" ~ "sensitivity",
    .metric == "spec" ~ "specificity",
    TRUE ~ .metric)) 

sv_film_res_metrics %>% 
  ggplot(aes(.metric, mean, fill = .metric)) +
  geom_errorbar(aes(ymin = mean - std_err,
                    ymax = mean + std_err),
                width=.2, size = 2)  +
  geom_point(shape = 21, show.legend = FALSE,
             size = 10)  +
  scale_y_continuous(labels = 
                         paste0(seq(0.5, 1, 0.015) * 100, "%"), 
                     breaks = seq(0.5, 1, 0.015)) +
  labs(y = "10 folds cross validated value", x = NULL)  +
  scale_fill_manual(values = c("#f9b4ab", "#264e70",
                               "#679186", "#fdebd3"))
```

Using the `vip` package we can compute what are the features that influence the most our prediction. As we could expect *documentary* is the most important word but other word seems very interesting like:

-   ***evil***

-   ***Christmas***

-   ***magic***

```{r fig.height=10}
library(vip)
films_train %>%  mutate(listed_in = factor(listed_in)) 

films_train$listed_in %>%  unique()

films_train$listed_in <- 
    fct_relevel(films_train$listed_in , 
                ref = "Children & Family Movies")

set.seed(345)
films_imp <- films_wf %>%
  add_model(svm_spec) %>%
  fit(films_train) %>%
  extract_fit_parsnip() %>%
  vi(
    method = "permute", nsim = 10,
    target = "listed_in", metric = "accuracy",
    pred_wrapper = kernlab::predict, 
    train = juice(films_rep %>%  prep())
  )

films_imp %>% 
  slice_max(Importance, n = 25) %>%
  mutate(
    Variable = str_remove(Variable, "tfidf_description_"),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(Importance, Variable, color = Importance)) +
  geom_errorbar(aes(xmin = Importance - StDev, 
                    xmax = Importance + StDev),
    alpha = 0.5, size = 1.3) +
  geom_point(size = 3) +
  theme(legend.position = "top") +
  labs(y = NULL, color = "Feature importance") + 
  scale_color_viridis_c(option = "C")  +
  guides(color = guide_colourbar(barwidth = 20, 
                                 title.position = "top")) 
```

We can finally test out model on the test set and see if we overfitted or not.

```{r}
films_final <- films_wf %>%
  add_model(svm_spec) %>%
  last_fit(films_split, 
           metrics = metric_set(roc_auc, accuracy, 
                                sens, spec))
```

Looking at the metrics and the confusion matrix it seams that we do not overfitted to much.

```{r}
collect_metrics(films_final) %>% 
  select(.metric, .estimate)  %>% 
  mutate(.metric = case_when(
    .metric == "roc_auc" ~ "roc auc",
    .metric == "sens" ~ "sensitivity",
    .metric == "spec" ~ "specificity",
    TRUE ~ .metric
  )) %>% 
    ggplot(aes(.metric, .estimate, fill = .metric)) +
    geom_segment(aes(xend = .metric, yend = 0.5), 
               color = "grey50") +
    geom_point(shape = 21, show.legend = FALSE,
               size = 10)  +
    scale_y_continuous(labels = paste0(seq(0.5, 1, 0.015) *100, 
                                     "%"), 
                       breaks = seq(0.5, 1, 0.015)) +
    labs(y = NULL, x = NULL)  +
    scale_fill_manual(values = c("#f9b4ab", "#264e70", 
                               "#679186", "#fdebd3"))
```

```{r fig.height=10, fig.width=10}
mat <- collect_predictions(films_final) %>% 
  janitor::clean_names()   %>%
  conf_mat(pred_class, listed_in)   

mat$table %>% 
  data.frame()  %>% 
  mutate(Prediction = 
             str_replace(Prediction, 
                         pattern = " & Family Movies",
                         "\n& Family \nMovies"),
         Truth = 
             str_replace(Truth, 
                         pattern = " & Family Movies",
                         "\n& Family \nMovies")) %>% 
  
  ggplot(aes(Prediction, Truth, fill = Freq))  +
  geom_bin2d(bins = 100, color = "black") +
  geom_text(aes(label = Freq), size = 10) +
  scale_fill_continuous(type = "viridis")  +
  labs(x = "Predicted class", y = "True class", 
       fill = "Frequency") +
  theme(legend.position = "top") +
  coord_fixed() +
  guides(fill = guide_colourbar(barwidth = 15), 
         text  = element_text(size = 10)) 
```

# End 

We have made through this analysis. We have learn that text data are very sparse, linear models like PCA and linear SVM works very well with those type of data.
